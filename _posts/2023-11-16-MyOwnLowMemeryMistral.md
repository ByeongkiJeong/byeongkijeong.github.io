---
layout: post
title: 16GB ë¨ì—ì„œ Mistral(LLM) êµ¬ë™í•˜ê¸° (feat. ë§¥ë¶í”„ë¡œ M1 pro 16GB)
comments: true
---

# LLM(Large Language Model)
LLMì´ ë­”ì§€, íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë­”ì§€ ë“±ë“±ì€ ì„¤ëª…í•˜ì§€ ì•Šê² ìŠµë‹ˆë‹¤.  
ë‹¤ë§Œ ì´ë¦„ì— ë§ê²Œ ë„ˆë¬´ ë¬´ê²ì£ ..íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆê°€ 7B/16B/30B ë“±ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆì§€ë§Œ, ê°€ì¥ ì‘ì€ ëª¨ë¸ë„ Weight íŒŒì¼ë§Œ ì‹­ìˆ˜ê¸°ê°€ì— ìœ¡ë°•í•©ë‹ˆë‹¤.  
(ì—¬ë‹´ì´ì§€ë§Œ sLLM-Small Large Language Model ì´ë¼ëŠ” ë§ ì›ƒê¸°ì§€ ì•Šë‚˜ìš”...)  

>ì‘ì€ê±¸ë¡œ ìœ ëª…í•œ Mistral-7B v0.1ë„ 15ê¸°ê°€...(9.94+5.06)
![ì‘ì€ê±¸ë¡œ ìœ ëª…í•œ Mistral-7B v0.1ë„ 15ê¸°ê°€...](../img/post_img/2023-11-16-MyOwnLLM/mistral_size.png)
  

ì €ëŠ” 16GB RAMì„ ê°€ì§„, M1 pro ë§¥ë¶ ê¹¡í†µì„ ì”ë‹ˆë‹¤. ê·¼ë° ì—¬ê¸°ì—ì„œ LLMì„ ëŒë ¤ë³´ê³  ì‹¶ì–´ì„œ ëª‡ê°€ì§€ ì‹œë„í•´ ë´¤ëŠ”ë°ìš”.  
ìš°ì„  ëŒ€í¬ì ìœ¼ë¡œ Huggingface + Mistral ì¡°í•©ì„ ì‹œë„í•´ ë´¤ëŠ”ë°, ê°€ìƒë©”ëª¨ë¦¬ê¹Œì§€ ì•½ 45ê¸°ê°€ì¯¤ ì“°ê³  ì£½ì–´ë²„ë¦¬ë”ë¼êµ¬ìš”.  
ê·¸ë˜ì„œ ë‘˜ëŸ¬ë³´ë‹¤, [ìš”ëŸ°](https://medium.com/@mne/run-mistral-7b-model-on-macbook-m1-pro-with-16gb-ram-using-llama-cpp-44134694b773) ê¸€ì´ ìˆê¸¸ë˜ í•œë²ˆ ë”°ë¼í•´ë³´ê³ ,
ì› ê¸€ì—ì„œ ì¡°ê¸ˆ ë¶ˆì¹œì ˆí•œ ë¶€ë¶„ì„ ì •ë¦¬í•´ë³´ì•˜ìŠµë‹ˆë‹¤.  

# ì–´ë–»ê²Œ 16GB ì‘ì€ ë©”ëª¨ë¦¬ì— LLMì„ ì˜¬ë¦´ ìˆ˜ ìˆë‚˜?
ê´€ë ¨í•´ì„œëŠ” [ì›ê¸€](https://medium.com/@mne/run-mistral-7b-model-on-macbook-m1-pro-with-16gb-ram-using-llama-cpp-44134694b773)ì„ ì°¸ê³ í•˜ë©´ ë‘ê°€ì§€ í¬ì¸íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤.  
1. ì–‘ìí™”(Quantization): ëª¨ë¸ ì‚¬ì´ì¦ˆ ì¤„ì´ê¸° ê¸°ë³¸ì´ì£ , ì‰½ê²Œë§í•˜ë©´ Weightì˜ ë°ì´í„° íƒ€ì…ì„ ë°”ê¿”ì£¼ëŠ” ê²ƒ ì…ë‹ˆë‹¤.
2. GGUF(GPT-Generated Unified Format): GPTì™€ ê°™ì€ LLMì„ ë¹ ë¥´ê²Œ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ ë°”ì´ë„ˆë¦¬ íŒŒì¼ í¬ë§· ì…ë‹ˆë‹¤.

ìœ„ ë‘ê°€ì§€ë¥¼ í†µí•´, ìƒëŒ€ì ìœ¼ë¡œ ì €ì‚¬ì–‘ì˜ í™˜ê²½ì—ì„œë„ ê°„ë‹¨íˆ LLMì„ ëŒë ¤ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ì–´ë–»ê²Œ í•˜ëŠ”ë°?
ê°„ë‹¨íˆ ì„¸ ë‹¨ê³„ë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
ì–‘ìí™” ëœ ì €ìš©ëŸ‰ Weightë¥¼ ë°›ê³ , ì´ë¥¼ êµ¬ë™í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì„œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.

1. ì–‘ìí™” ëœ Weightë¥¼ ë‹¤ìš´ë°›ëŠ”ë‹¤.
2. llama-cpp ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¹Œë“œí•œë‹¤.
3. êµ¬ë™ğŸ˜€!

## 1. ì–‘ìí™” ëœ Weightë¥¼ ë‹¤ìš´ë°›ëŠ”ë‹¤.
ì €ëŠ” ì›ê¸€ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ Mistralë¡œ í• ê±´ë°ìš”.  
The Bloke AI ì—ì„œ ì—¬ëŸ¬ê°€ì§€ ë²„ì „ìœ¼ë¡œ ì–‘ìí™” ëœ Mistralì˜ Weightë¥¼ ê³µìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.  
ì €ëŠ” [mistral-7b-instruct-v0.1.Q6_K.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q6_K.gguf?download=true) ë¥¼ ë‹¤ìš´ë°›ì•˜ìŠµë‹ˆë‹¤.

> Completion ëª¨ë¸ì€ [ì—¬ê¸°](https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF)ì—ì„œ,  
> Instruct ëª¨ë¸ì€ [ì—¬ê¸°](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)ì—ì„œ ë‹¤ìš´ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
> ![the Bloke AI](../img/post_img/2023-11-16-MyOwnLLM/mistral_gguf.png)

## 2. llama-cpp ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¹Œë“œí•œë‹¤.
llama-cppëŠ” GGUF í¬ë§·ì˜ íŒŒì¼ì„ êµ¬ë™í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì…ë‹ˆë‹¤. ì´ë¦„ì—ì„œ ë³´ì´ì‹¤ë§Œí¼ ì²˜ìŒì—ëŠ” LLaMaë¥¼ êµ¬ë™í•˜ê¸° ìœ„í•´ì„œ ë§Œë“¤ì–´ì§„ ê²ƒ ê°™ê³ , C++ë¡œ ì§œì—¬ì§„ê²ƒ ê°™ì§€ë§Œ,,,ì–´ì¨Œë“  ê·¸ê±¸ Pythonì—ì„œ ì“°ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

bashì—ì„œ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ìˆœì„œëŒ€ë¡œ ì…ë ¥í•˜ì‹œë©´ ë©ë‹ˆë‹¤.  
ëŒ€ì¶© ë³´ì‹œë©´ ì•„ì‹œê² ì§€ë§Œ, githubì—ì„œ ë³µì‚¬í•´ì™€ì„œ, Apple siliconì— ë§ê²Œ ë¹Œë“œí•˜ëŠ” ê²ƒ ë¿ì…ë‹ˆë‹¤.  
ë¹Œë“œ ì´í›„ llama-cpp í´ë” ì•„ë˜ì— **./build/bin/main**ì´ ìƒì„±ë˜ì—ˆë‹¤ë©´, ì •ìƒì ìœ¼ë¡œ ë¹Œë“œ ëœ ê²ƒì…ë‹ˆë‹¤.
```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
mkdir build
cd build
cmake .. -DCMAKE_APPLE_SILICON_PROCESSOR=arm64 
make -j
```

## 3. êµ¬ë™ğŸ˜€!
1ë²ˆì—ì„œ ë‹¤ìš´ë°›ì€ .gguf íŒŒì¼ì„, ì ë‹¹í•œ ê²½ë¡œë¡œ ì˜®ê²¨ì¤ë‹ˆë‹¤.  
ì €ëŠ” **llama.cpp** í´ë” ì•„ë˜ **weights** í´ë”ì— ë„£ì–´ë‘ì—ˆìŠµë‹ˆë‹¤.  

   
ê·¸ëŸ¬ë©´, llama-cppë„ ì˜ ë¹Œë“œ ë˜ì—ˆê³ , weightíŒŒì¼ë„ ì˜ ì¡´ì¬í•œë‹¤ë©´, ì•„ë˜ ëª…ë ¹ì–´ë¡œ ì‹¤í–‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.  
(ì¤‘ê°„ì— weight ê²½ë¡œëŠ” ì•Œì•„ì„œ ìˆ˜ì •í•´ì£¼ì„¸ìš” âœï¸)

```bash
./build/bin/main --color --model "./weights/mistral-7b-instruct-v0.1.Q6_K.gguf" -t 7 -b 24 -n -1 --temp 0 -ngl 1 -ins
```

ê·¸ëŸ¬ë©´ ì•„ë˜ì™€ ê°™ì´ ì‹¤í–‰ ë©ë‹ˆë‹¤. 
![ì‹¤í–‰ í™”ë©´](../img/post_img/2023-11-16-MyOwnLLM/mistral_run.png)

ì¸ì‚¬ë„ ì˜ ë°›ì•„ì£¼ë„¤ìš”.
![hi!](../img/post_img/2023-11-16-MyOwnLLM/mistal_hi.png)

### 3+@. Python ì±—ë´‡ìœ¼ë¡œ ë§Œë“¤ê¸°(ì›ê¸€ì— ì—†ëŠ” ë‚´ìš©!ğŸ˜)
LLMì˜ í™œìš©ì„±ì—ì„œ í•µì‹¬ì€ Context ì…ë ¥ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.  
ì´ì „ì˜ ëŒ€í™”ë¥¼ ì…ë ¥í•´ì„œ ë¬¸ë§¥ì„ íŒŒì•…í•˜ê²Œ í•˜ëŠ” ëŠ¥ë ¥ì¸ë°ìš”.

Mistralì˜ Prompt Templateì€ ì‚¬ìš©ìì˜ ëª…ë ¹ì–´ëŠ” ```[INST]```ì™€ ```[/INST]```ë¡œ ê°ì‹¸ê³ , ì´ì „ ëŒ€í™”ëŠ” ```<s>```ì™€ ```</s>```ë¡œ ê°ì‹¸ë©´ ë©ë‹ˆë‹¤.  
```python
text = """
<s>[INST] What is your favourite condiment? [/INST]
Well, I'm quite partial to a good squeeze of fresh lemon juice. </s>
[INST] Do you have mayonnaise recipes? [/INST]"""
```
  

> (```<s>```ì™€ ```</s>```ëŠ” ê°ê° Begin-of-string, End-of-stringì„ ì˜ë¯¸í•˜ëŠ”ë°, ì´ì „ëŒ€í™”ë¥¼ ê°ì‹¸ëŠ”ê²Œ ë§ëŠ”ì§€ëŠ” í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.. ì œëŒ€ë¡œ ëœ ì„¤ëª…ì„ ì°¾ì§€ ëª»í–ˆëŠ”ë° ëŒ€ë¶€ë¶„ ì˜ˆì‹œê°€ ê·¸ëŸ°ì‹ìœ¼ë¡œ ê°ì‹¸ê¸¸ë˜ ì¼ë‹¨ ê·¸ëŸ°ê°€ë³´ë‹¤ í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.)

OpenAIì˜ GPT APIë¥¼ ì¨ë³´ì‹  ë¶„ì´ë¼ë©´ ì•„ì‹¤í…ë°, ê±°ê¸°ì—ëŠ” System / User / Assistantë¡œ Roleì˜ ì¢…ë¥˜ê°€ êµ¬ë¶„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  

Mistralì—ëŠ” User(=INST) / Assistantë§Œ ìˆì§€ë§Œ, ì‚¬ì „ì— ëª‡ê°€ì§€ Contextë¥¼ ì •ì˜í•´ë‘ë©´, ìœ ìš©í•˜ê²Œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

ì˜ˆë¥¼ ë“¤ì–´, ì½”ë“œì— ì•„ë˜ì™€ ê°™ì€ ë‚´ìš©ì„ ë¯¸ë¦¬ ë„£ì–´ë‘ë©´, í•œêµ­ì–´ë¥¼ ì£¼ë¡œ ì´ì•¼ê¸° í•˜ëŠ” LLMì´ ë©ë‹ˆë‹¤.  
(ë¬¼ë¡  ì„±ëŠ¥ì´ë‚˜ ì¶”ë¡ ì‹œê°„ì€ ì˜ì–´ê°€ í›¨ì”¬ ë‚«ë”ë¼êµ¬ìš”. í•œêµ­ì–´ëŠ” ì¶”ë¡ ì‹œê°„ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ê¸¸ì–´ì§€ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.)

```Python
text = """
<s> [INST] You are a native speaker of korean. Now you only response with korean. [/INST]
ë„¤, ì €ëŠ” ì´ì œë¶€í„° í•œêµ­ì–´ë¡œ ì´ì•¼ê¸° í•©ë‹ˆë‹¤.
"""
```

ì¶”ê°€ì ìœ¼ë¡œ, ì•„ë˜ ì½”ë“œë¥¼ ë³„ë„ íŒŒì¼ë¡œ ë§Œë“¤ì–´ë‘ê³  bashì—ì„œ ì‹¤í–‰í•˜ë©´, ì• Contextë¥¼ ê¸°ì–µí•˜ëŠ” ìƒíƒœë¡œ ì±—ë´‡ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
from llama_cpp import Llama
model = "./weights/mistral-7b-instruct-v0.1.Q6_K.gguf"
llm = Llama(model_path=model, n_ctx=8192, n_batch=512, n_threads=8, n_gpu_layers=2, verbose=False, seed=1004)

messages = []

base_message = """
<s> [INST] You are a native speaker of korean. Now you only response with korean. [/INST]
ë„¤, ì €ëŠ” ì´ì œë¶€í„° í•œêµ­ì–´ë¡œ ì´ì•¼ê¸° í•©ë‹ˆë‹¤.
"""

while True:
    user = input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”(-1: end, 0: clear): ")
    if user == '-1': break
    if user == '0': 
        messages=[base_message]
        continue
    message = '\n'.join(messages) + '</s>'
    messages.append(f"[INST] {user} [/INST]")
    output = llm(message + messages[-1], echo=True, stream=False, max_tokens=4096)
    print(output['usage'])
    output = output['choices'][0]['text'].replace(message + messages[-1], '')
    print(output)
    system = output
```